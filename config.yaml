# Model and Dataset Configuration
model: "wav2vec2"
dataset: "librispeech"

# Training Hyperparameters
local_steps: 128
lr: 3e-4
outer_lr: 0.8
warmup_steps: 500
total_steps: 16000

# Batch Size Configuration
per_device_train_batch_size: 64
batch_size: 128
max_batch_size: 128

# Optimization Settings
optim_method: "sgd"

# Checkpoint Configuration
checkpoint_path: "wav2vec2_librispeech_checkpoint.pth"
checkpoint_interval: 5

# Device Configuration
device: "cuda"
gpu_type: "amd"

# Distributed Backend Configuration (optional)
pgroup_backend: "gloo"  # Options: nccl, gloo, ucc (auto-selects if not specified)

# WandB Configuration
wandb_project_name: "test"
wandb_group: "test_new_metrics"
wandb_run_id: "wav2vec2_librispeech4"

# Advanced Features
heterogeneous: true
compression_decay: 0.95
compression_topk: 32

# Experiment Metadata
experiment_description: "GPT-Neo-X training on C4 using DiLoCo distributed training"
experiment_tags:
  - "gpt_neo_x"
  - "c4"
  - "diloco"
  - "distributed"
  - "heterogeneous"
  - "llm"
seed: 42
wandb_logging: true

# torch.compile settings
compile_model: false
compile_backend: "inductor"
compile_mode: "default"

# Hugging Face Upload Configuration
hf_upload: true
trained_model_hf_name: 'exalsius/test_model_name'  # Set to "your-username/your-model-name" to enable upload