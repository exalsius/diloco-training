# Model and Dataset Configuration
model: "gcn"
dataset: "ogbn_arxiv"

# Training Hyperparameters
local_steps: 128
lr: 3e-4
outer_lr: 0.7
warmup_steps: 500
total_steps: 40000

# Batch Size Configuration
per_device_train_batch_size: 64
batch_size: 512

# Optimization Settings
optim_method: "sgd"

# Checkpoint Configuration
checkpoint_path: "gcn_obgn_checkpoint.pth"
checkpoint_interval: 5

# Device Configuration
device: "cuda"

# WandB Configuration
wandb_project_name: "test"
wandb_group: "2nodes-homogeneous"
wandb_run_id: "gcn1"

# Advanced Features
heterogeneous: true
compression_decay: 0.95
compression_topk: 32

# Experiment Metadata
experiment_description: "GCN training on LibriSpeech using DiLoCo distributed training"
experiment_tags:
  - "gcn"
  - "ogbn_arxiv"
  - "diloco"
  - "distributed"
  - "heterogeneous"
  - "llm"
seed: 42
wandb_logging: true

# torch.compile settings
compile_model: true
compile_backend: "inductor"
compile_mode: "default"