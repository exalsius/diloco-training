# Model and Dataset Configuration
model: "gpt-neo-x"
dataset: "c4"

# Training Hyperparameters
local_steps: 4
lr: 3e-4
outer_lr: 0.7
warmup_steps: 500
total_steps: 20

# Batch Size Configuration
per_device_train_batch_size: 64
batch_size: 512
min_batch_size: 32
max_batch_size: 512

# Optimization Settings
optim_method: "sgd"
async_communication: false

# Checkpoint Configuration
checkpoint_path: "gcn_obgn_checkpoint.pth"
checkpoint_interval: 3

# Device Configuration
device: "cuda"

# WandB Configuration
wandb_project_name: "test"
wandb_group: "2nodes-homogeneous"
wandb_run_id: "gcn1"

# Hardware
heterogeneous: true
group_perc_variance: 0.15

# Compression Settings Demo
compression_decay: 0.95
compression_topk: 32

# Experiment Metadata
experiment_description: "GCN training on LibriSpeech using DiLoCo distributed training"
experiment_tags:
  - "gcn"
  - "ogbn_arxiv"
  - "diloco"
  - "distributed"
  - "heterogeneous"
  - "llm"
seed: 42
wandb_logging: true

# torch.compile settings
compile_model: true
compile_backend: "inductor"
compile_mode: "default"